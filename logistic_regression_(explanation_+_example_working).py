# -*- coding: utf-8 -*-
"""Logistic Regression (Explanation + Example working)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nd-DTky7aP-cuIB2WL5BxoCTkVtrBCdr

Defined the load_dataset() from the course instructions
"""

import numpy as np
import h5py
    
    
def load_dataset():
    train_dataset = h5py.File('/content/drive/My Drive/Colab Notebooks/Logistic Regression as a Neural Network/datasets/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('/content/drive/My Drive/Colab Notebooks/Logistic Regression as a Neural Network/datasets/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

"""Got the packages needed"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage

# %matplotlib inline

"""Got the data -> One important thing to remeber is to add the data when I restart this colab // go to the left tab and click on the option "mount drive""""

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

"""Example of a picture"""

# Example of a picture
index = 28 # choose some index and see it
plt.imshow(train_set_x_orig[index])
print ("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") +  "' picture.")

m_train = train_set_x_orig.shape[0]
m_test = test_set_x_orig.shape[0]
num_px = train_set_x_orig.shape[1]
print("M train: ", m_train, "\nM test: ", m_test, "\nNumPx: ", num_px)

"""We'll expand the pixels of the image in a way that we no longer have that 3d representation of the image, instead we're going to have a big matrix of size $numPx \cdot numPx \cdot 3$, which is that collapse of the 3d matriz we have into a vector.
It is going to be represented just like in the video course:

$\begin{bmatrix}
x_1^{1} & x_1^{2} & \cdots & x_1^{m}\\
x_2^{1} & x_2^{2} & \cdots & x_2^{m}\\
\vdots & \vdots & \ddots & \vdots\\
x_{n_x}^1 & x_{n_x}^2 & \cdots & x_{n_x}^{m}
\end{bmatrix}$

in such a way that we have Nx rows (the x values of each test case) and m colums, where each column represents a testcase
"""

train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T
print(train_set_x_flatten.shape)
print(test_set_x_flatten.shape)

"""Now we need to normalize the data in order to make it more feasible and appropriate for the gradient descent to work"""

train_set_x = train_set_x_flatten/255
test_set_x = test_set_x_flatten/255

"""Now we're going to build ourselves the sigmoid function
It is important to remeber what the sigmoid function actually is:
$\sigma(x) = \dfrac{1}{1 + e^{-x}}$
from which we can derive its use, that is to map the values to the interval $[0..1]$
"""

def sigmoid(x):
  # Note that x might be a numpy matrix, and we apply the function to each element
  s = 1 / (1 + np.exp(-x))
  return s

def init(dim):
  w = np.zeros((dim,1))
  b = 0
  return w, b

"""Doing a recap on what we need to calulate for our linear regression in the foward propagation to work:
$z^i = w^t x^i + b$ ($w^t$ defines the transposal of w, for us to be able to multiply it by $x^i$ in the way we want) (remember that $w^t$ and $x^i$ are vectors, so the multiplication defined in there is their dot product)and this z value is the value we get applying the weights to each one of those inputs and then adding to it the bias.
Next in the computation graph there is the value of a, which is the sigmoid of z: $a^i = \sigma(z^i)$. Going even further in the computational graph we need to remeber of the loss function, which takes as parameters the prediction of our model for that test set and the set comparing values and then outputs a value which we want to minimize: $L(a^i, y^i) = y \cdot ln(a) + (1-y) \cdot ln(1-a)$. Which is used because the value of $y^i$ can only assume two possibilities, which means that if $y^i = 1$ we'll take the first value and we'll want to make $a^i$ as big as possible, on the other case it is similiar but we want to make $a^i$ as small as possible (remember that those are inside a sigmoid, which means that they're mapped in the interval $[0..1]$. It is also good because it is convex and gives us a global minima the gradient descent can search for.
There also is another function that is defined on top of the loss function for each that case which is the cost function of that set of weights and biases on the entire dataset, which is defines as: $J = \frac{-1}{m} \cdot (\sum_{j=1}^{m}L(a^i, y^i))$, which is nothing more than the average of the loss functions of every test case.

That came out too big so I'll just write out the formulas:

$z^i = w^t x^i + b$

$a^i = \sigma(z^i)$

$L(a^i, y^i) = y \cdot ln(a) + (1-y) \cdot ln(1-a)$

$J = \frac{-1}{m} \cdot (\sum_{i=1}^{m}L(a^i, y^i))$

Those are the formulas we need to understand how the work is being done on every single testcase on the foward propagation fase, but in the actual code we vectorize it to make it faster due to the efficiency of the numpy library, instead of working them out manually using for loops. The code for those values will look like written below

The inputs are a column vector (aka matrix of shape = $(n_x,1)$) W, which are the weights of associated with each input. A matrix of shape = $(n_x, m)$, which is just like described above a bunch of testcases on each column (m columns means that there are m test cases) -> The input data of each test.
A real number $b$, which is the bias of that neuron. A row vector (aka matrix of shape = $(1,m)$) which has the expected values of each test case.

We also want to calculate in this propagate function all the derivatives/gradients needed for the gradient descent to work, so let's remind how to find them:
First of all we need to understand the chain rule in the computation graph, which means that we want to find the derivative of the loss function with respect to each weight and to the bias. To do so we use the chain rule to know that 

$\dfrac{\partial f(v(a))}{\partial a} = \dfrac{\partial f(v(a))}{\partial v(a)} \cdot \dfrac{\partial v(a)}{\partial a}$.

And so we can generate in a backwards sense the derivatives of the loss function with respect to each value.

What actually happens is: we calculate the derivate of $L$ with respect to $a \rightarrow (sigmoid(z))$, which turns out to be a weird number that I don't remember, then we calculate the derivative of a with respect to z, which is the derivative of a sigmoig which is: $\sigma'(x) = \sigma(x)\cdot(1-\sigma(x))$
Combining those two we get $\dfrac{\partial L(y, a)}{\partial z} = a - y$.
So we have a clean value for dz we can use.
Now we want to calculate the value of$\dfrac{\partial L(y, a)}{\partial w^i}$. To do so we simply get by using the chain rule 

$\dfrac{\partial L(y, a)}{\partial w^i} = \dfrac{\partial L(y, a)}{\partial z} \cdot \dfrac{\partial z}{\partial w^i} = x^i \cdot (a - y)$

A pretty much the same ideia works to find the derivative of b, but it is only one value this time instead of multiple values.
"""

def propagate(w, b, X, Y):
  m = X.shape[1]
  
  ### FOWARD PROPAGATION ###
  z = np.dot(w.T, X) + b
  a = sigmoid(z)

  # print(w.shape)
  # print(X.shape)

  # print(w)
  # print(X)
  # print(z)
  # print(a)

  # A is now a row vector (shape = (1,m)), so we need to transpose it to be
  # able to do the matrix multiplication correctly
  
  cost = (-1/m)*(np.dot(Y, np.log(a).T) + np.dot((1-Y), np.log(1-a).T))
  
  cost = np.squeeze(cost) # Turns the cost into a single variable
  
  assert(cost.shape == ())

  ### BACKWARD PROPAGATION (FIND DERIVATIVES/GRADIENTS) ###

  dz = a - Y
  dw = (1/m)*np.dot(X, dz.T)
  db = (1/m)*np.sum(dz)

  # print(dw.shape)

  grads = {"dw": dw,
           "db": db}

  return grads, cost

w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])
grads, cost = propagate(w, b, X, Y)
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
print ("cost = " + str(cost))

"""Now we're onto the step of implmenting the gradient descent. Which we can run multiple times but what we do on every iteration is simply run a foward and then backpropagation (in the function we coded above) and get the value of the cost function (just to illustrate the improvements of the regression), and most importantly the derivatives in that point of the values of every weight (they're represented in the code as a column vector which contains the derivatives of every weight in the network -> it is in the format of a column vector, just like the original weights vector), we also take the derivative of b. Using those we update the values of the weights and of b in the following manner:
$w = w - \alpha \cdot dw$, where dw is the derivatives we calculated and alpha is the learning rate. The same is applied to b.
"""

def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False, period = 100):
  
  costs = []

  for i in range(num_iterations):
    grads, cost = propagate(w, b, X, Y)
    dw = grads["dw"]
    db = grads["db"]

    if print_cost and i%period == 0:
      print("Cost after the %i iteration %f" %(i, cost))

    if i%period == 0:
      costs.append(cost)
    
    w = w - learning_rate * dw
    b = b - learning_rate * db
    learning_rate /= 1.0005 # I added this line to make the learning rate not constant

    params = {"w": w,
              "b": b}
    
    grads = {"dw": dw,
             "db": db}

  return params, grads, costs

params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = True)

print ("w = " + str(params["w"]))
print ("b = " + str(params["b"]))
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))

"""Now we need to get what is our predictions based on the value of a( if a <= 0.5 it is 0, otherwise it is 1)"""

def predict(w, b, X):
  z = np.dot(w.T, X)
  a = sigmoid(z)
  Y_prediction = np.round(a, decimals=0)
  Y_prediction = Y_prediction.astype(int)
  assert(Y_prediction.shape == (1, X.shape[1]))
  return Y_prediction

w = np.array([[0.1124579],[0.23106775]])
b = -0.3
X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])
print ("predictions = " + str(predict(w, b, X)))

"""Now we'll wrap everything together to build our model"""

def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = True, period=100):
  w, b = init(X_train.shape[0])
  params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost, period)
  
  w = params["w"]
  b = params["b"]
  dw = grads["dw"]
  db = grads["db"]

  # we now get what our model predicts for each test case
  Y_prediction_test = predict(w, b, X_test)
  Y_prediction_train = predict(w, b, X_train)

  # print accuracy
  print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
  print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

  d = {"costs": costs,
       "Y_prediction_test": Y_prediction_test, 
       "Y_prediction_train" : Y_prediction_train, 
       "w" : w, 
       "b" : b,
       "learning_rate" : learning_rate,
       "num_iterations": num_iterations}
  
  return d

"""Use this to train the model adjusting the parameters"""

d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 3000, learning_rate = 0.003, print_cost = True, period=500)

"""Select an index of the test set to test"""

index = 13
plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))
print ("y = " + str(test_set_y[0,index]) + ", you predicted that it is a \"" + classes[d["Y_prediction_test"][0, index]].decode("utf-8") +  "\" picture.")

"""Plot the costs over the iterations"""

costs = np.squeeze(d['costs'])
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(d["learning_rate"]))
plt.show()

"""Add any image (jpg or jpeg) to the images folder and then change the name to see what is the prediction of the algorithm for it

"""

# I added this because the warnings here we annoying
import warnings
warnings.filterwarnings('ignore')


## START CODE HERE ## (PUT YOUR IMAGE NAME) 
my_image = "gato.jpg"   # change this to the name of your image file 
## END CODE HERE ##

# We preprocess the image to fit your algorithm.
fname = "/content/drive/My Drive/Colab Notebooks/Logistic Regression as a Neural Network/images/" + my_image
image = np.array(ndimage.imread(fname, flatten=False))
image = image/255.
my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T
my_predicted_image = predict(d["w"], d["b"], my_image)

plt.imshow(image)
print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")